import gradio as gr
import requests
import json
import time
import psutil


# This function will call the local API and handle the prompt and model input parameters to generate text completions
# The API will return the prompt tokens, completion tokens, and total tokens used during the generation process, as well as the time it took to respond
# These details will be used later to calculate response time and tokens per second, which are metrics to gauge performance
def call_local_api(prompt, model):
    url = "http://localhost:11434/v1/completions"  # The URL is pointing to a local API running on port 11434, which will handle requests for generating completions
    headers = {
        "Content-Type": "application/json"
    }  # Set the header to inform the server that we are sending JSON data in the request body

    # The data we send to the API is a JSON object with the 'model' we want to use (e.g., "qwen:0.5b") and the 'prompt' which is the input query
    data = {
        "model": model,  # The selected model that will be used to process the prompt (e.g., "qwen:0.5b", "qwen:1.8b", or "qwen:4b")
        "prompt": prompt,  # The prompt is the user-provided question or input that the model will process and generate a response for
    }

    # Capture the start time before sending the request. This is used to calculate how long the request takes
    start_time = (
        time.time()
    )  # This function returns the current time in seconds since the epoch, and it's used to measure the API response time

    # Send a POST request to the API with the prompt and model data in JSON format
    response = requests.post(url, headers=headers, data=json.dumps(data))

    # Capture the time right after the request is completed to calculate the total response time
    end_time = (
        time.time()
    )  # This is the end time in seconds right after the API response is received
    response_time = (
        end_time - start_time
    )  # Calculate how long the API took to respond (in seconds) by subtracting start_time from end_time

    # If the response from the API was successful (HTTP status code 200), we will extract the necessary data from the response JSON
    if response.status_code == 200:
        result = (
            response.json()
        )  # Parse the response from the API as JSON. This will contain information about the completion, tokens, etc.

        # Extract the generated text from the response if it exists. If the 'choices' key is not present, return a default message "No text found"
        completion_text = (
            result["choices"][0]["text"] if "choices" in result else "No text found"
        )

        # Extract the token usage information, which tells us how many tokens were used for the prompt, completion, and total.
        # This helps in calculating the efficiency of the model. If no token data is present, it defaults to 0
        prompt_tokens = result["usage"].get(
            "prompt_tokens", 0
        )  # Tokens used for the input prompt
        completion_tokens = result["usage"].get(
            "completion_tokens", 0
        )  # Tokens used for the generated completion text
        total_tokens = result["usage"].get(
            "total_tokens", 0
        )  # Total tokens used, which is the sum of prompt_tokens and completion_tokens

        # Return the text generated by the model, along with token statistics and the time taken for the response
        return (
            completion_text,
            prompt_tokens,
            completion_tokens,
            total_tokens,
            response_time,
        )
    else:
        # If there was an error (non-200 status code), return an error message and zero values for tokens and time
        return (
            f"Error: {response.status_code}, {response.text}",
            0,
            0,
            0,
            0,
        )  # This helps with debugging if the API call fails


# This function is used to get the current memory usage of the process running this script
# It helps to monitor how much memory the local API or this script is consuming during execution
def get_memory_usage():
    process = (
        psutil.Process()
    )  # Get the current process (the script itself) using psutil
    mem_info = (
        process.memory_info()
    )  # Retrieve memory usage statistics for the current process
    return mem_info.rss / (
        1024 * 1024
    )  # Return the memory usage in megabytes (RSS: Resident Set Size, which is the memory currently used by the process)


# This is the main function that handles user input, tracks the conversation history, and updates the table with performance data
# It takes the user's input, sends it to the API, appends the result to the chat history, and calculates various performance metrics such as memory usage and tokens/s
def chatgpt_clone(input, history, model, table_history):
    history = (
        history or []
    )  # If there's no existing chat history, initialize it as an empty list. This will store the conversation between the user and the chatbot
    table_history = (
        table_history or []
    )  # Similarly, if there's no existing table data, initialize it as an empty list. This will store performance metrics in a tabular format

    # Flatten the chat history into a list of strings, then append the new input prompt from the user
    s = list(
        sum(history, ())
    )  # Sum up the history to merge tuples into a flat list (flattening tuples into one list)
    s.append(input)  # Append the new user input to the conversation history
    inp = " ".join(
        s
    )  # Join the list into a single string with spaces, forming the complete input prompt

    # Call the local API to get the response for the given input prompt and the selected model
    output, prompt_tokens, completion_tokens, total_tokens, response_time = (
        call_local_api(inp, model)
    )

    # Get the current memory usage of the process in megabytes to track how much memory the process is consuming
    memory_usage = get_memory_usage()

    # Calculate the number of tokens processed per second by dividing the total tokens used by the response time
    tokens_per_second = (
        total_tokens / response_time if response_time > 0 else 0
    )  # Only perform division if the response time is greater than zero

    # Append the new prompt and completion to the conversation history
    history.append(
        (input, output)
    )  # Store the conversation as tuples of (user_input, model_output) in the history

    # Prepare a new row of data for the table, including token counts, response time, memory usage, and tokens/s metric
    new_data = [
        prompt_tokens,
        completion_tokens,
        total_tokens,
        round(response_time, 3),
        round(memory_usage, 2),
        round(tokens_per_second, 2),
    ]

    # Append the new performance metrics (new_data) to the table history, which tracks all the previous results
    table_history.append(new_data)

    # Return the updated history (conversation), the updated history (for the chatbot), and the updated table with performance data
    return history, history, table_history, table_history


# Create the Gradio interface, which will allow users to interact with the chatbot and see the performance metrics in real time
block = (
    gr.Blocks()
)  # Blocks is the main container in Gradio that allows us to organize our interface components

# Inside the block, define the layout and components of the interface
with block:
    # Create a markdown header to display the title of the application in the center
    gr.Markdown("<h1><center>Build Your Own Chatbot with Local LLM Model</center></h1>")

    # Create a row layout to contain the chatbot and table side by side
    with gr.Row():
        chatbot = gr.Chatbot(
            label="Chatbot"
        )  # The chatbot component shows the conversation between the user and the model
        # Create a DataFrame (table) to display token and performance data
        # Headers define the columns, and datatype sets the expected data type for each column (numbers in this case)
        table = gr.DataFrame(
            headers=[
                "prompt_tokens",
                "completion_tokens",
                "total_tokens",
                "response_time (s)",
                "memory_usage (MB)",
                "tokens/s",
            ],
            datatype=["number"] * 6,
        )

    # Add a textbox where the user can input their prompt/question for the chatbot
    message = gr.Textbox(
        placeholder="Ask anything to the AI assistant...", label="Your Prompt"
    )

    # Add a dropdown menu for users to select which model they want to use (e.g., "qwen:0.5b", "qwen:1.8b", or "qwen:4b")
    model_choice = gr.Dropdown(
        choices=["qwen:0.5b", "qwen:1.8b", "qwen:4b"],
        value="qwen:0.5b",
        label="Choose Model",
    )

    # Initialize a state object to store the conversation history
    state = gr.State()  # This state keeps track of the

    submit_button = gr.Button("Submit")
    submit_button.click(
        chatgpt_clone,  # Function to call on click
        inputs=[message, state, model_choice, state],  # Inputs to the function
        outputs=[chatbot, state, table, table],  # Outputs to update
    )

    # Launch the Gradio interface
    block.launch()
